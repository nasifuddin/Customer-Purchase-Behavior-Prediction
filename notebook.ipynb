{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the csv\n",
    "retail_csv = pd.read_csv(\"data/Online Retail Data Set.csv\", encoding='unicode_escape')\n",
    "retail_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the data type, shape and overall description of the dataset \n",
    "retail_csv.info(), retail_csv.shape, retail_csv.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking missing value per column\n",
    "retail_csv.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we can notice that:\n",
    "- there are missing values in `Description` and `CustomerID` \n",
    "- `InvoiceDate` needs to be converted to date-time object\n",
    "- `UnitPrice` and `Quantity` have negative values\n",
    "- `CustomerID` can be a categorial variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Handling the missing values first by droping rows where Description or CustomerID is missing\n",
    "retail_csv = retail_csv.dropna(subset=['Description','CustomerID'])\n",
    "retail_csv.isnull().sum(), retail_csv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert InvoiceDate column to date-time object\n",
    "retail_csv['InvoiceDate'] = pd.to_datetime(retail_csv['InvoiceDate'], dayfirst='True')\n",
    "retail_csv.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dealing with the negative value from UnitPrice and Quantity\n",
    "retail_csv = retail_csv[(retail_csv['Quantity'] > 0) & (retail_csv['UnitPrice'] > 0)]\n",
    "retail_csv.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retail_csv['InvoiceNo'].value_counts(), retail_csv['StockCode'].value_counts(), retail_csv['CustomerID'].value_counts(), retail_csv['Country'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will encode `InvoiceNo`, `StockCode`, `CustomerID`, `Country` as categorial variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding the categorial variables\n",
    "categorical_columns = ['InvoiceNo', 'StockCode', 'Description', 'Country','CustomerID']\n",
    "label_encoders = {col:LabelEncoder() for col in categorical_columns}\n",
    "\n",
    "for col in categorical_columns:\n",
    "    retail_csv[col] = label_encoders[col].fit_transform(retail_csv[col])\n",
    "    \n",
    "#Feature scaling for UnitPrice and Quantity\n",
    "# scaler = StandardScaler()\n",
    "# retail_csv[['Quantity','UnitPrice']] = scaler.fit_transform(retail_csv[['Quantity','UnitPrice']])\n",
    "\n",
    "#check the new data\n",
    "retail_csv.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's see the summary of our preprocessed data\n",
    "retail_summary = retail_csv.describe()\n",
    "retail_null_check = retail_csv.isnull().sum()\n",
    "print(retail_summary)\n",
    "print(f\"\\nChecking for missing values in each column: \\n{retail_null_check}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some observation:\n",
    "- `Quantity` and `UnitPrice` are properly scaled and has no negative value. but both have some outliers\n",
    "- The categorial columns (`InvoiceNo`, `StockCode`, `Description`, `Country`) are encoded properly.\n",
    "- has no missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing outliers of UnitPrice\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x=retail_csv['UnitPrice'])\n",
    "plt.title('Boxplot of UnitPrice')\n",
    "plt.xlabel('UnitPrice')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing outliers of Quantity\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x=retail_csv['Quantity'])\n",
    "plt.title('Boxplot of Quantity')\n",
    "plt.xlabel('UnitPrice')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retail_csv.to_csv(\"preprocessed_retail.csv\",index = \"False\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unitprice_percentile = retail_csv['UnitPrice'].quantile(0.73)\n",
    "quantity_percentile = retail_csv['Quantity'].quantile(0.73)\n",
    "\n",
    "retail_filtered = retail_csv[\n",
    "    (retail_csv['UnitPrice'] <= unitprice_percentile) &\n",
    "    (retail_csv['Quantity'] <= quantity_percentile)\n",
    "]\n",
    "\n",
    "retail_filtered.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing outliers of UnitPrice\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x=retail_filtered['UnitPrice'])\n",
    "plt.title('Boxplot of UnitPrice')\n",
    "plt.xlabel('UnitPrice')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing outliers of Quantity\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x=retail_filtered['Quantity'])\n",
    "plt.title('Boxplot of Quantity')\n",
    "plt.xlabel('UnitPrice')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the dataframe after filtering\n",
    "retail_filtered.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't need `InvoiceNo`,`CustomerID`,`Description` columns for the prediction. So, We will drop them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retail_filtered = retail_filtered.drop(columns=[\"InvoiceNo\",\"Description\"])\n",
    "retail_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograms for all numerical features\n",
    "numerical_features = retail_filtered.select_dtypes(include=['int64', 'float64']).columns\n",
    "for feature in numerical_features:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.histplot(retail_filtered[feature], kde=True)\n",
    "    plt.title(f'Distribution of {feature}')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot for 'UnitPrice' vs 'Quantity'\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.scatterplot(x='UnitPrice', y='Quantity', data=retail_filtered)\n",
    "plt.title('UnitPrice vs Quantity Scatter Plot')\n",
    "plt.xlabel('UnitPrice')\n",
    "plt.ylabel('Quantity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Correlation heatmap\n",
    "# correlation_matrix = retail_filtered.corr()\n",
    "# plt.figure(figsize=(12, 8))\n",
    "# sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "# plt.title('Correlation Heatmap')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Node:\n",
    "#     def __init__(self, feature=None, threshold=None, left=None, right=None,*,value=None):\n",
    "#         self.feature = feature\n",
    "#         self.threshold = threshold\n",
    "#         self.left = left\n",
    "#         self.right = right\n",
    "#         self.value = value\n",
    "        \n",
    "#     def is_leaf_node(self):\n",
    "#         return self.value is not None\n",
    "\n",
    "\n",
    "# class DecisionTree:\n",
    "#     def __init__(self, min_samples_split=2, max_depth=100, n_features=None):\n",
    "#         self.min_samples_split=min_samples_split\n",
    "#         self.max_depth=max_depth\n",
    "#         self.n_features=n_features\n",
    "#         self.root=None\n",
    "\n",
    "#     def fit(self, X, y):\n",
    "#         self.n_features = X.shape[1] if not self.n_features else min(X.shape[1],self.n_features)\n",
    "#         self.root = self._grow_tree(X, y)\n",
    "\n",
    "#     def _grow_tree(self, X, y, depth=0):\n",
    "#         n_samples, n_feats = X.shape\n",
    "#         n_labels = len(np.unique(y))\n",
    "\n",
    "#         # check the stopping criteria\n",
    "#         if (depth>=self.max_depth or n_labels==1 or n_samples<self.min_samples_split):\n",
    "#             leaf_value = self._most_common_label(y)\n",
    "#             return Node(value=leaf_value)\n",
    "\n",
    "#         feat_idxs = np.random.choice(n_feats, self.n_features, replace=False)\n",
    "\n",
    "#         # find the best split\n",
    "#         best_feature, best_thresh = self._best_split(X, y, feat_idxs)\n",
    "\n",
    "#         # create child nodes\n",
    "#         left_idxs, right_idxs = self._split(X[:, best_feature], best_thresh)\n",
    "#         left = self._grow_tree(X[left_idxs, :], y[left_idxs], depth+1)\n",
    "#         right = self._grow_tree(X[right_idxs, :], y[right_idxs], depth+1)\n",
    "#         return Node(best_feature, best_thresh, left, right)\n",
    "\n",
    "\n",
    "#     def _best_split(self, X, y, feat_idxs):\n",
    "#         best_gain = -1\n",
    "#         split_idx, split_threshold = None, None\n",
    "\n",
    "#         for feat_idx in feat_idxs:\n",
    "#             X_column = X[:, feat_idx]\n",
    "#             thresholds = np.unique(X_column)\n",
    "\n",
    "#             for thr in thresholds:\n",
    "#                 # calculate the information gain\n",
    "#                 gain = self._information_gain(y, X_column, thr)\n",
    "\n",
    "#                 if gain > best_gain:\n",
    "#                     best_gain = gain\n",
    "#                     split_idx = feat_idx\n",
    "#                     split_threshold = thr\n",
    "\n",
    "#         return split_idx, split_threshold\n",
    "\n",
    "\n",
    "#     def _information_gain(self, y, X_column, threshold):\n",
    "#         # parent entropy\n",
    "#         parent_entropy = self._entropy(y)\n",
    "\n",
    "#         # create children\n",
    "#         left_idxs, right_idxs = self._split(X_column, threshold)\n",
    "\n",
    "#         if len(left_idxs) == 0 or len(right_idxs) == 0:\n",
    "#             return 0\n",
    "        \n",
    "#         # calculate the weighted avg. entropy of children\n",
    "#         n = len(y)\n",
    "#         n_l, n_r = len(left_idxs), len(right_idxs)\n",
    "#         e_l, e_r = self._entropy(y[left_idxs]), self._entropy(y[right_idxs])\n",
    "#         child_entropy = (n_l/n) * e_l + (n_r/n) * e_r\n",
    "\n",
    "#         # calculate the IG\n",
    "#         information_gain = parent_entropy - child_entropy\n",
    "#         return information_gain\n",
    "\n",
    "#     def _split(self, X_column, split_thresh):\n",
    "#         left_idxs = np.argwhere(X_column <= split_thresh).flatten()\n",
    "#         right_idxs = np.argwhere(X_column > split_thresh).flatten()\n",
    "#         return left_idxs, right_idxs\n",
    "\n",
    "#     def _entropy(self, y):\n",
    "#         hist = np.bincount(y)\n",
    "#         ps = hist / len(y)\n",
    "#         return -np.sum([p * np.log(p) for p in ps if p>0])\n",
    "\n",
    "\n",
    "#     def _most_common_label(self, y):\n",
    "#         counter = Counter(y)\n",
    "#         value = counter.most_common(1)[0][0]\n",
    "#         return value\n",
    "\n",
    "#     def predict(self, X):\n",
    "#         return np.array([self._traverse_tree(x, self.root) for x in X])\n",
    "\n",
    "#     def _traverse_tree(self, x, node):\n",
    "#         if node.is_leaf_node():\n",
    "#             return node.value\n",
    "\n",
    "#         if x[node.feature] <= node.threshold:\n",
    "#             return self._traverse_tree(x, node.left)\n",
    "#         return self._traverse_tree(x, node.right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class RandomForestClassifier():\n",
    "    \n",
    "#     def __init__(self, n_trees = 10, max_depth =10, min_samples_split = 2, n_feature = None):\n",
    "#         self.n_trees = n_trees\n",
    "#         self.max_depth = max_depth\n",
    "#         self.min_sample_split = min_samples_split\n",
    "#         self.n_feature = n_feature\n",
    "#         self.trees = []\n",
    "        \n",
    "#     def fit(self, X, y):\n",
    "#         self.trees = []\n",
    "#         for _ in range(self.n_trees):\n",
    "#             tree = DecisionTree(max_depth=self.max_depth,\n",
    "#                                 min_samples_split = self.min_sample_split,\n",
    "#                                 n_features = self.n_feature)\n",
    "#             X_sample, y_sample = self._bootstrap_samples(X,y)\n",
    "#             tree.fit(X_sample, y_sample)\n",
    "#             self.trees.append(tree)\n",
    "            \n",
    "#     # def predict(self, X):\n",
    "#     #     for tree in self.trees:\n",
    "#     #         predictions =np.array(tree.predict(X))\n",
    "#     #     tree_preds = np.swapaxes(predictions, 0,1)\n",
    "#     #     predictions = np.array([self._common_label(pred) for pred in tree_preds])\n",
    "#     #     return predictions\n",
    "    \n",
    "#     def predict(self, X):\n",
    "#         predictions = np.array([tree.predict(X) for tree in self.trees])\n",
    "#         tree_preds = np.swapaxes(predictions, 0, 1)\n",
    "#         predictions = np.array([self._common_label(pred) for pred in tree_preds])\n",
    "#         return predictions\n",
    "    \n",
    "#     def _bootstrap_samples(self, X, y):\n",
    "#         n_samples = X.shape[0]\n",
    "#         idx = np.random.choice(n_samples, n_samples, replace = True)\n",
    "#         return X[idx],y[idx]\n",
    "    \n",
    "#     def _common_label(self, y):\n",
    "#         counter = Counter()\n",
    "#         most_common = counter.most_common(1)[0][0]\n",
    "#         return most_common"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retail_filtered.to_csv(\"retail_filtered.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = retail_filtered.drop(['Quantity','InvoiceDate'], axis =1).values\n",
    "y = retail_filtered['Quantity'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print (X_train.shape)\n",
    "print (X_test.shape)\n",
    "print (y_train.shape)\n",
    "print (y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# rfr = RandomForestRegressor(n_estimators=1000)\n",
    "# rfr.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = rfr.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll be making a random forest regressor from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Decision Node used in the Decision Tree\n",
    "class DecisionNode:\n",
    "    def __init__(self, feature_index=None, threshold=None, left=None, right=None, value=None):\n",
    "        self.feature_index = feature_index\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n",
    "\n",
    "# Decision Tree Regressor\n",
    "class DecisionTreeRegressor:\n",
    "    def __init__(self, min_samples_split=2, max_depth=float('inf')):\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        self.root = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.root = self._grow_tree(X, y)\n",
    "        \n",
    "    def _grow_tree(self, X, y, depth=0):\n",
    "        num_samples, num_features = X.shape\n",
    "        if num_samples >= self.min_samples_split and depth < self.max_depth:\n",
    "            best_feat, best_thresh = self._best_split(X, y, num_features)\n",
    "            if best_feat is not None:\n",
    "                indices_left = X[:, best_feat] < best_thresh\n",
    "                X_left, y_left = X[indices_left], y[indices_left]\n",
    "                X_right, y_right = X[~indices_left], y[~indices_left]\n",
    "                left = self._grow_tree(X_left, y_left, depth + 1)\n",
    "                right = self._grow_tree(X_right, y_right, depth + 1)\n",
    "                return DecisionNode(feature_index=best_feat, threshold=best_thresh, left=left, right=right)\n",
    "        return DecisionNode(value=np.mean(y))\n",
    "\n",
    "    def _best_split(self, X, y, num_features):\n",
    "        best_feat, best_thresh = None, None\n",
    "        best_mse = float('inf')\n",
    "        current_value = np.mean(y)\n",
    "        current_mse = np.mean((y - current_value) ** 2)\n",
    "        for feat in range(num_features):\n",
    "            thresholds = np.unique(X[:, feat])\n",
    "            for thresh in thresholds:\n",
    "                indices_left = X[:, feat] < thresh\n",
    "                y_left, y_right = y[indices_left], y[~indices_left]\n",
    "                if len(y_left) > 0 and len(y_right) > 0:\n",
    "                    mse = (len(y_left) * np.mean((y_left - np.mean(y_left)) ** 2) + len(y_right) * np.mean((y_right - np.mean(y_right)) ** 2)) / len(y)\n",
    "                    if mse < best_mse:\n",
    "                        best_mse = mse\n",
    "                        best_feat = feat\n",
    "                        best_thresh = thresh\n",
    "        return best_feat, best_thresh\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._predict(inputs, self.root) for inputs in X])\n",
    "    \n",
    "    def _predict(self, inputs, node):\n",
    "        if node.value is not None:\n",
    "            return node.value\n",
    "        if inputs[node.feature_index] < node.threshold:\n",
    "            return self._predict(inputs, node.left)\n",
    "        return self._predict(inputs, node.right)\n",
    "\n",
    "# Random Forest Regressor\n",
    "class RandomForestRegressor:\n",
    "    def __init__(self, n_estimators=5, min_samples_split=2, max_depth=float('inf')):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.trees = []\n",
    "        for _ in range(self.n_estimators):\n",
    "            tree = DecisionTreeRegressor(self.min_samples_split, self.max_depth)\n",
    "            bootstrap_indices = np.random.choice(np.arange(len(X)), size=len(X), replace=True)\n",
    "            tree.fit(X[bootstrap_indices], y[bootstrap_indices])\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        tree_preds = np.array([tree.predict(X) for tree in self.trees])\n",
    "        return np.mean(tree_preds, axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = RandomForestRegressor()\n",
    "\n",
    "regressor.fit(X_train, y_train)\n",
    "y_pred = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Calculate metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Scatter plot of actual vs predicted values\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.plot([y.min(), y.max()], [y.min(), y.max()], '--k')\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predictions')\n",
    "plt.title('True vs Predicted Values')\n",
    "\n",
    "# Text box for metrics\n",
    "textstr = f'MSE: {mse:.2f}\\nR2: {r2:.2f}'\n",
    "props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "plt.text(0.05, 0.95, textstr, fontsize=14, verticalalignment='top', bbox=props)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
